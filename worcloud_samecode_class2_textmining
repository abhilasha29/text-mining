#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Sep  9 12:10:14 2017

@author: abhilashakumari
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Sep  9 11:22:24 2017
#nltk is stored in  this location : /Users/abhilashakumari/nltk_data
@author: abhilashakumari
"""
import os
os.chdir("/Users/abhilashakumari/downloads/")
import json
import string
import nltk              #natural language tool package
from nltk import word_tokenize, FreqDist
from nltk.corpus import stopwords #stopwords that comes with nltk

# We'll process just one article for illustration

# Set folder path to the directory where the files are located
# And open the file(s). For example:
#folder_path = 'D:\MTECH\TextMining\Day2'
#data =  open(os.path.join(folder_path, "Article_1.json"), "r")

# Or open the JSON data file from your current working directory
data =  open("Article_1.json", "r")

#in article json we have 3 keywords, url, text and id 

# Load in the JSON object in the file
jdata = json.load(data) #read from file and turn everything into json object

# Extract the URL and the Text from within the JSON object
url=jdata['URL']
url

text=jdata['Text']
text
type(text)
len(text)
text[9300:]

# Conver the free text into tokens
tokens = word_tokenize(text)
type(tokens)

# A little exploration: How many words in this article? How many unique words?
# Any single character words?
len(tokens)
tokens[:20] #gives first 21 words 
unique = set(tokens)
len(unique)
len(tokens)/len(unique) # average that a word appears in the document
sorted(unique)[:30]  #sort in alphabetical order and select 30

      #some of the words are very short so we can check how many single length word is there
single=[w for w in unique if len(w) == 1 ]  #list comprehension ,looping in a list , here if lentght of word is 1 then we keep in the list otherwise remove it! w -> word
len(single)
single

# Frequency distribution of the words
tokens.count('gluten') #shows no of times gluten appears in text, 
fd = nltk.FreqDist(tokens) #freqdist is for quick counting inside the list , (called frequentdistance)
fd.most_common(50) #shows 50 most common things, shows most frequent words first. this doc tells about gluten and food
fd.plot(50) 

# How long are the words?
fd_wlen = nltk.FreqDist([len(w) for w in unique]) #for every word in list w preparing list of character appearing in the list [output 1:20 , lengthof word:frequency]
fd_wlen

# What about bigrams and trigrams?
bigr = nltk.bigrams(tokens[:10]) #bigram , in order to preserve context(i.e sequence we select 2 words at a time),takes first word second word together , 2nd &3rd word togethr
trigr = nltk.trigrams(tokens[:10])#trigram 
tokens[:10]
list(bigr)
list(trigr)

# Back to text preprocessing: remove punctuations
tokens_nop = [ t for t in tokens if t not in string.punctuation ]
print(tokens[:50])
print(tokens_nop[:50])
len(tokens)
len(tokens_nop)
len(set(tokens_nop))

# Convert all characters to Lower case
tokens_lower=[ t.lower() for t in tokens_nop ]
print(tokens_lower[:50])
len(set(tokens_lower)) #converting to lower case , total no tokens doesnot change but no of unique tokens change

# Create a stopword list from the standard list of stopwords available in nltk

stop = stopwords.words('english') # stop word is applied only after case lowering
print(stop)

# Remove all these stopwords from the text
tokens_nostop=[ t for t in tokens_lower if t not in stop ] # if string is not in lowr list, keep it
print(tokens_nostop[:50])
len(tokens_lower)
len(tokens_nostop)
FreqDist(tokens_nostop).most_common(50) #after all preprocessing again se frequecy distribution 

# Now, let's do some Stemming!
# There are different stemmers available in Python. Let's take a look at a few

# The most popular stemmer
porter = nltk.PorterStemmer() #getting instance of porter stammer
tokens_porter=[ porter.stem(t) for t in tokens_nostop ] 
print(tokens_nostop[:50]) #before stemming
print(tokens_porter[:50]) #after stemming

# The Lancaster Stemmer - developed at Lancaster University
lancaster = nltk.LancasterStemmer() # tokens becom more shorter in lancaster package, this is not good as  excessive chopping of may chop two different words into same word and introduce a lot of noise so its not good here
tokens_lanc = [ lancaster.stem(t) for t in tokens_nostop ] 
print(tokens_lanc[:50])

# The snowball stemmer -  which supports 13 non-English languages as well!

snowball = nltk.SnowballStemmer('english') #snowball is newer so is preferred 
tokens_snow = [ snowball.stem(t) for t in tokens_nostop ]
print(tokens_snow[:50])
len(set(tokens_snow))


# Now, for Lemmatization, which converts each word to it's corresponding lemma, use the Lemmatizer provided by nltk
wnl = nltk.WordNetLemmatizer() #deals with grammatical variations, noun variations and verb variations. we get full words here as it is based on dictionary information 
tokens_lem = [ wnl.lemmatize(t) for t in tokens_nostop ]
print(tokens_lem[:50])
len(set(tokens_lem))

# Check the lemmatization results. Why are some words not lemmatized?
# The reason is it needs to know the POS of the words. The default is 'n'.
# We'll learn how to do POS tagging later.
wnl.lemmatize('absorbed', pos = 'v')

# Let's use Snowball Stemmer's result.
# Further cleaning: filter off anything with less than 3 characters
nltk.FreqDist(tokens_snow).most_common(100) #since now weare not concentrating on lemmatizer so lets concentrate on snowball
tokens_clean = [ t for t in tokens_snow if len(t) >= 3 ] # as most good words are greater than 3
len(tokens_snow)
len(tokens_clean)
nltk.FreqDist(tokens_clean).most_common(50)
fd_clean = nltk.FreqDist(tokens_clean)

# Join the cleaned tokens back into a string.
# Why? Because some functions we'll use later require string as input.
text_clean=" ".join(tokens_clean) #many package takes input as string so we are making string out of the clean word (in sytax its space.join and then clean string is passed)


# ==== Installation of wordcloud package
# 1. download wordcloud‑1.3.2‑cp36‑cp36m‑win_amd64.whl from http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud 
# 2. Copy the file to your current working directory
# 3. Open command prompt from Tools
#install using pip install wordcloud
# 4. python -m pip install wordcloud-1.3.2-cp36-cp36m-win_amd64.whl
import wordcloud
from wordcloud import WordCloud, ImageColorGenerator
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image

# 1. Simple cloud
# Generate a word cloud image
# Take note that this function requires text string as input
wc = WordCloud(background_color="white").generate(text_clean) #to create word cloud through string by setting back ground as white, default is white
wc
# Display the generated image:
# the matplotlib way:

plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()

wc.to_file("example.png")

# We can also generate directly from the frequency information
wc2 = WordCloud(background_color="white")
wc2.generate_from_frequencies(fd_clean)
plt.imshow(wc2, interpolation='bilinear')
plt.axis("off")
plt.show()

# 2. Cloud with customized shape and color
mask = np.array(Image.open("./fly.png"))
image_colors = ImageColorGenerator(mask)#creating mask by telling word clod as its boundary of the image (which is butterfly here)

wc3 = WordCloud(background_color='white', mask=mask).generate(text_clean)

plt.imshow(wc3.recolor(color_func=image_colors)) # we say the function imshow that we are going to use colors of image as our wordclod background
plt.axis("off")
plt.show()
